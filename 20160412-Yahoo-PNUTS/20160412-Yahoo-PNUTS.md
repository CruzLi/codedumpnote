#PNUTS特点

1.  数据中心被分布在全球多个地方，每个数据中心都有所有的用户数据，用户使用最近的数据中心进行访问。
2.  存储的是KV结构数据，数据使用schema-free的设计，即非固定表结构，valua使用json格式，删除和更新操作需要指定key。
3.  高可用性和容错，单个数据中心的失效不影响整个服务。
4.  优先保证可用性，因此优先提供弱一致性的数据方法模式。当然也提供更严格的数据访问模型，下面会提到。

#一致性模型
PNUTS对每条记录都加上版本号，规则如下：

*   每次新插入数据时会生成递增一个大版本号。
*   每次更新（包括删除）已经存在的数据递增一个小版本号。

![per-record timeline](./1.png "per-record timeline")

使用记录的版本号，PNUTS提供了如下几种API：

##Read-any
系统可能返回任意版本的数据，这里就有可能有过期的数据。然而对很多业务而言，一致性的要求并不那么强，降低一致性带来的就是可用性的提高。

##Read-critical(required version)
返回一个参数要求的版本号的数据。这种API面向那些在写入数据成功之后，需要立即看到这次写入对应结果的业务。

##Read-latest
读到最新的数据，这个API的一致性要求是最高的，相应的可用性就不如前两个API。

##Write
提供了ACID的保证，这个API用于比如用户更新资料的场景。

##Test-and-set-write(required version)
类似于CAS功能，即只有在当前数据满足对应版本号时才进行数据的写入。


#架构
PNUTS的每个数据中心称为一个region，每一个region都有全部用户的数据，不可避免会出现不同的region之间的数据一致问题。每个region包括如下几个组件：

![system architecture](./Figure1.png "system architecture")

*   storage unit（以下简称SU）:存储单元。PNUTS上存储数据的基本单位是tablet，一个storage unit上有多个tablet，每个tablet可能有几百M或者几G的大小，包含千到万级别的记录。tablet可能有两种类型的table，一种是hash表，基于一般的unix文件系统实现；另一种是有序表（ordered table），基于Mysql的Innodb引擎实现，因为这种存储引擎基于主键进行排序。
*   router：router相当于region的门户，要查找一个数据在哪个storage unit上存储，首先要经过router。router查询数据的规则，根据不同类型的表有所区别，如下图所示，如果是有序表，将主键分为不同的区间，router存储了不同区间所在的tablet；而对于hash表，首先将计算查找key的hash值，将这个hash值映射到范围[0,2^n）之间，同样的此时router也只需要存储每个区间所在的tablet即可。
*   tablet controller：上面提到的router存放的键到区间的映射关系只是一份临时数据，而router需要定期的向这里的tablet controller拉取最新的映射数据。如果router上的数据是过期的，那么可能请求会被定向到错误的tablet上，此时会返回一个错误，这样也驱动router到tablet controller上获取最新的映射关系数据。这里也可以看到，这样的设计中，router是状态很轻的服务，及时宕机重新启动一个拉取最新的映射关系即可，并不会造成太大影响。通常tablet controller会在一个集群中存在两份，但是不会成为系统的瓶颈，因为这个组件并不在关键的请求路径上。
*   Yahoo message broker（以下简称YMB）：一个用于PUB/SUB发布订阅消息的组件，用于同步数据，后面会详细谈到这个组件。它是几个组件中唯一一个用于集群中通信的组件。

![Interval mappings](./Figure2.png "Interval mappings")

#备份与一致性
YMB是一个pub/sub的消息系统，PNUTS通过YMB进行数据的同步，任何数据的更新只有在提交给YMB之后才认为是提交成功的，而YMB在接收到提交的数据之后，会异步的把这份新的数据修改同步给其他region的YMB。YMB在接收到更新数据之后，至少会先写入到两个服务器的log，根据这份log将这次修改同步到所有其它备份中，只有在全部备份都更新了数据之后，这个log才可以被删除。这里的疑问在于，如果需要写两份服务器log才认为是在YMB这里提交成功，那么看起来修改数据的延时是比较高的。

为了保证对每条记录的更新都是按序的，每条记录都有一个对应的负责这条记录的主备份。即使在同一个表上的不同记录，它的主备份可能在不同的服务器上。这利用了数据局部性原理，因为所有的region上都有全量的数据，而并不是每份数据在这个region上都会被经常修改，所以不同的数据可能有不同的主备份服务器，Yahoo的统计数据表明，每条记录85%的修改都在同一个数据中心上。因为数据修改的粒度，每种业务可能是不一样的，所以这里提供的是针对记录级别的主备份，而不是针对表级别的。同时，一条记录的主备份可能会发生迁移，比如用户用威斯康辛州搬家到加州，他的数据主备份也会相应的转移到那边的region上。PNUTS同样在每条记录中增加了一个隐藏字段表示在某个region中修改数据的次数，如果大于这个次数，那么认为这条记录的主备份应该发生迁移，会将这条记录迁移到对应的region上。

YMB在同步修改时，都会被修改先发到主备份对应的服务器上，由它来决定修改的顺序，从而保证了数据修改的顺序都是一致的。如果在非主备份上的修改，每条记录都会有一个隐藏字段记录它的主备份，那么这个修改会根据这个字段重定向到主备份上。

有上面的准备，下面简单描述一下整个更新数据的流程，其中部分都是根据论文的描述来猜测的：

1.  客户端发送更新请求到该客户端对应的region的router上。
2.  router将这个请求发送给本region的SU1。
3.  SU1查找这条数据的key在region R2上。
4.  SU1转发这个请求到R2的router上。
5.  R2的router查询到这个key对应在本region的SU2，于是发送请求给本region的SU2。
6.  SU2发送请求给本SU的YMB。
7.  YMB写日志备份此次修改，应答给客户端。
8.  YMB将这次修改同步给所有的region。
9.  所有region完成本地关于此条数据的修改。

这里的问题在于，这次修改是由非主备份所在的region发起的，所以流程比较长，但是如果是本region发起的修改，相应的前面几步并不会发生。另外同步给所有region的步骤是在应答客户端之后，只需要YMB写入磁盘日志成功之后就会应答了。

可以看到，每个region都具有全量的数据，既保证了不会因为一个region的失效导致整个服务失效，也根据局部性原理，给用户提供了更好的使用体验。









