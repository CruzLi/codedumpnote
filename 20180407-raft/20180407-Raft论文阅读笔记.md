Raft论文阅读笔记

论文 [CONSENSUS: BRIDGING THEORY AND PRACTICE](https://ramcloud.stanford.edu/~ongaro/thesis.pdf) 阅读笔记

#基本算法
## server数据状态及通信RPC

注：这一小节拷贝自 [maemual/raft-zh_cn](https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md)

**状态**：

|状态|所有服务器上持久存在的|
|-------|------|
|currentTerm | 服务器最后一次知道的任期号（初始化为 0，持续递增）|
|votedFor | 在当前获得选票的候选人的 Id|
| log[] | 日志条目集；每一个条目包含一个用户状态机执行的指令，和收到时的任期号 |

|状态|所有服务器上经常变的|
|-------|------|
| commitIndex| 已知的最大的已经被提交的日志条目的索引值|
| lastApplied| 最后被应用到状态机的日志条目索引值（初始化为 0，持续递增）|

| 状态 | 在领导人里经常改变的 （选举后重新初始化）|
|----|--------|
| nextIndex[] | 对于每一个服务器，需要发送给他的下一个日志条目的索引值（初始化为领导人最后索引值加一）|
| matchIndex[] | 对于每一个服务器，已经复制给他的日志的最高索引值|


**附加日志 RPC**：

由领导人负责调用来复制日志指令；也会用作heartbeat

| 参数 | 解释 |
|----|----|
|term| 领导人的任期号|
|leaderId| 领导人的 Id，以便于跟随者重定向请求|
|prevLogIndex|新的日志条目紧随之前的索引值|
|prevLogTerm|prevLogIndex 条目的任期号|
|entries[]|准备存储的日志条目（表示心跳时为空；一次性发送多个是为了提高效率）
|leaderCommit|领导人已经提交的日志的索引值|

| 返回值| 解释|
|---|---|
|term|当前的任期号，用于领导人去更新自己|
|success|跟随者包含了匹配上 prevLogIndex 和 prevLogTerm 的日志时为真|

接收者实现：

1. 如果 `term < currentTerm` 就返回 false
2. 如果日志在 prevLogIndex 位置处的日志条目的任期号和 prevLogTerm 不匹配，则返回 false
3. 如果已经存在的日志条目和新的产生冲突（索引值相同但是任期号不同），删除这一条和之后所有的
4. 附加任何在已有的日志中不存在的条目
5. 如果 `leaderCommit > commitIndex`，令 commitIndex 等于 leaderCommit 和 新日志条目索引值中较小的一个

**请求投票 RPC**：

由候选人负责调用用来征集选票

| 参数 | 解释|
|---|---|
|term| 候选人的任期号|
|candidateId| 请求选票的候选人的 Id |
|lastLogIndex| 候选人的最后日志条目的索引值|
|lastLogTerm| 候选人最后日志条目的任期号|

| 返回值| 解释|
|---|---|
|term| 当前任期号，以便于候选人去更新自己的任期号|
|voteGranted| 候选人赢得了此张选票时为真|

接收者实现：

1. 如果`term < currentTerm`返回 false
2. 如果 votedFor 为空或者就是 candidateId，并且候选人的日志至少和自己一样新，那么就投票给他

## Leader选举
server启动时，初始状态都是Follower。而leader通过定期发送心跳消息来告知每个follower该leader还存活。一旦follower在一段时间（这个时间段称为选举超时（election timeout））内都没有接收到leader的心跳消息，那么该follower认为该leader已经不可用，将重新进行选举。

follower进行一次新的选举时，首先递增当前的term，然后切换到candidate状态，接着向集群内其他所有server发送RequestVote RPC消息。一个candidate将保持在该状态，一直到如下几种情况发生：

1.  该candidate赢得选举。
2.  其他server被选举为新的leader。
3.  在选举超时到来之前，没有任何server赢得选举。


以下就这三种情况进行解释。

1.  一个candidate在集群里超过半数的server都投票给它时赢得选举，而每个server在一次选举里只能投票一次。
2.  在candidate进行选举的过程中，可能会收到来自其他server的AppendEntries RPC消息，如果该RPC里的term不小于该candidate当前的term，那么candidate认为该server是leader，切换到follower状态，选举结束。否则，candidate将拒绝该AppendEntries消息。
3.  第三种可能是，在其他candidate同时也进行选举的情况下，该candidate即没有赢得也没有输掉这次选举。在这种情况下，如果到了该candidate的下一次选举超时仍然没有新的leader产生，那么该candidate将再次递增term进行下一次新的选举。而为了避免同时进行选举的candidate这种情况频繁发生，每个server将会设置一个随机的选举超时时间（一般在150~300ms），这样各server的选举超时时间错开。


## 日志复制
当leader接收到客户端的请求时，每个请求都包含一条可被状态机执行的指令，此时leader会进行如下的操作：

1.  将该指令做为一条新的日志条目附件到leader自身的日志中去。
2.  向其他server发送AppendEntries RPC请求。
3.  当这个条目被安全复制（如何定义“安全复制”在下面讲述），leader将这个条目放入状态机执行，最后将执行的结果返回给客户端。

如果follower太慢，或者出现网络丢包等情况，leader将一直持续发送上面的RPC请求到follower。

日志的组织形式如下图3.5所示，一条日志一旦被集群中的超过半数server复制，那么被认为是“安全复制”，该日志就可以被提交（committed）。比如图3.5中的条目7及在之前的所有日志，而条目8就不是安全复制的，因为还没有超过半数的server复制了这条日志，该日志也就还不能提交。

![3.5](./pic/3.5.jpg)

一个日志条目被提交，这意味着在这条日志之前的所有日志条目也已经被提交，包括其他任期内其他leader提交的日志。
leader跟踪了最大的将会被提交的日志项的索引，并且索引值会被包含在未来的所有AppendEntries RPCs （包括heartbeat），这样其他的server才能最终知道leader的提交位置。一旦follower知道一条日志条目已经被提交，那么它也会将这个日志条目应用到本地的状态机中（按照日志的顺序）。

正常情况下，leader和follower的日志保持一致，这样AppendEntries RPC请求不会失败。但是在Leader崩溃的时候，可能会出现日志不一致的情况。

比如图3.6这样，展示了follower的日志可能和新的leader不同的方式。follower可能会丢失一些在新的leader中有的日志条目，它也可能拥有一些leader没有的日志条目，或者两者都发生。丢失或者多出日志条目可能会持续多个任期。

![3.6](./pic/3.6.jpg)


要使leader和follower两者的日志最终达成一致，leader维护针对每个follower的nextIndex索引值，这个值标识下一个需要发送该follower的日志索引。每次一个新的leader选举出来，该值都初始化为leader的最后一条日志索引+1.如果两者日志不一致，AppendEntries RPC请求就会失败，那么下一次leader再次给该follower发送AppendEntries请求时就会相应的将nextIndex减一发送，依次类推，直到到达两者日志一致的地方。

以上这个过程也可以优化：follower在拒绝AppendEntries消息时就返回冲突时的term以及index号，这样减少递减nextIndex索引进行试探的过程次数。

## 安全性
### 选举限制
不是所有的candidate都能赢得选举，只有那些candidate必须包含最新的日志才能赢得选举。在candidate向集群其他server发送RequestVote请求时，需要带上自己最后一条日志的term以及索引，只有大于server上最后一条日志的term和索引的才能赢得该server的投票，而只有赢得半数以上server投票的才能成为leader。

### 提交之前任期内的日志条目
提交之前任期的数据是不安全的，如下图8所示：

![5.8](./pic/5.8.png)

如图的时间序列展示了为什么领导人无法决定对老任期号的日志条目进行提交。在 (a) 中，S1 是领导者，部分的复制了索引位置 2 的日志条目。在 (b) 中，S1 崩溃了，然后 S5 在任期 3 里通过 S3、S4 和自己的选票赢得选举，然后从客户端接收了一条不一样的日志条目放在了索引 2 处。然后到 (c)，S5 又崩溃了；S1 重新启动，选举成功，开始复制日志。在这时，来自任期 2 的那条日志已经被复制到了集群中的大多数机器上，但是还没有被提交。如果 S1 在 (d) 中又崩溃了，S5 可以重新被选举成功（通过来自 S2，S3 和 S4 的选票），然后覆盖了他们在索引 2 处的日志。反之，如果在崩溃之前，S1 把自己主导的新任期里产生的日志条目复制到了大多数机器上，就如 (e) 中那样，那么在后面任期里面这些新的日志条目就会被提交（因为S5 就不可能选举成功）。 这样在同一时刻就同时保证了，之前的所有老的日志条目就会被提交。

为了消除图 8 里描述的情况，Raft 永远不会通过计算副本数目的方式去提交一个之前任期内的日志条目。只有领导人当前任期里的日志条目通过计算副本数目可以被提交；一旦当前任期的日志条目以这种方式被提交，那么由于日志匹配特性，之前的日志条目也都会被间接的提交。

### leader转让
有些情况下不得不进行leader的转让，比如：

1.  leader所在的主机需要进行维护。
2.  新的leader网络状况等更好。

转让leader流程：

1.  leader收到转让leader请求之后，保存转让目标节点的id。并且拒绝一切提交。
2.  leader向新的leader节点追加日志，直到两者匹配上为止。
3.  新旧leader的日志匹配上之后，旧leader向新的leader发送timeout协议。
4.  旧的leader收到timeout指令之后，开始一次新的竞选，并且强制自己成为新的leader。


# 集群成员发生变化
不能同时删除、增加超过一个server以上的server，因为这样可能会导致出现两个leader的情况，引发安全性问题。
在集群变化过程中，采用的是两阶段提交法。首先集群先切换到一个过渡性的配置上，称之为共同一致(join consensus)。一旦该共同一致被提交成功，集群就切换到新的配置上，共同一致是新老配置的结合：

* 日志条目被复制给集群中新、老配置的所有服务器。
* 新、旧配置的服务器都可以成为领导人。
* 达成一致（针对选举和提交）需要分别在两种配置上获得大多数的支持。

如下图9所示：

集群配置在复制日志中以特殊的日志条目来存储和通信；图9展示了配置转换的过程。当一个领导人接收到一个改变配置从 C-old 到 C-new 的请求，他会为了共同一致存储配置（图中的 C-old,new），以前面描述的日志条目和副本的形式。一旦一个服务器将新的配置日志条目增加到它的日志中，他就会用这个配置来做出未来所有的决定（服务器总是使用最新的配置，无论他是否已经被提交）。这意味着领导人要使用 C-old,new 的规则来决定日志条目 C-old,new 什么时候需要被提交。如果领导人崩溃了，被选出来的新领导人可能是使用 C-old 配置也可能是 C-old,new 配置，这取决于赢得选举的候选人是否已经接收到了 C-old,new 配置。在任何情况下， C-new 配置在这一时期都不会单方面的做出决定。

一旦 C-old,new 被提交，那么无论是 C-old 还是 C-new，在没有经过他人批准的情况下都不可能做出决定，并且领导人完全特性保证了只有拥有 C-old,new 日志条目的服务器才有可能被选举为领导人。这个时候，领导人创建一条关于 C-new 配置的日志条目并复制给集群就是安全的了。再者，每个服务器在见到新的配置的时候就会立即生效。当新的配置在 C-new 的规则下被提交，旧的配置就变得无关紧要，同时不使用新的配置的服务器就可以被关闭了。如图 11，C-old 和 C-new 没有任何机会同时做出单方面的决定；这保证了安全性。

![9](./pic/9.png)

> 图 9：一个配置切换的时间线。虚线表示已经被创建但是还没有被提交的条目，实线表示最后被提交的日志条目。领导人首先创建了 C-old,new 的配置条目在自己的日志中，并提交到 C-old,new 中（C-old 的大多数和  C-new 的大多数）。然后他创建 C-new 条目并提交到 C-new 中的大多数。这样就不存在  C-new 和 C-old 可以同时做出决定的时间点。

然而，除此之外，还有几个特殊情况需要考虑下：


## 向集群增加新的server

增加新server的时候，有可能出现该server的日志与当前集群的日志相差太远的情况，这种情况下，需要较长的时间来同步日志，此时如果把该server加入集群中，会增加集群不可用的风险，比如集群原来有3个server，本来可以容忍两个server的失败，但是在加入一个还在同步日志所以不可用的server之后，只需要再有一个server失败集群就不可用了。

因此，向集群增加新server时，新增了一个同步日志的流程，只有在同步日志成功之后，这个新的server才能加入到集群中参与选举等流程。

但是，也可能出现同步完日志了，马上又有新的数据产生，于是又需要同步新的日志的流程，以此下去，难以加入到集群中。

raft的做法是，等待一个固定的轮次（比如10轮），如果最新的这一轮在一次选举超时之内，那么认为该server的数据已经足够，即使再同步新的数据，也不会对集群的可用性造成明显的影响。

而如果到了这个轮次仍然没有同步完毕足够的数据，那么还是继续重新开始将轮次置为1继续同步下面的数据，以此类推，直到满足数据足够新的条件位置。

至于快速同步的做法，可以参考前面使用nextIndex进行快速同步leader与follower数据的做法。

## 删除当前集群leader

如下步骤：

1.  首先发送消息给集群中其他节点，告知leader节点被删除，这个新的配置变化，称为C(New)
2.  只有当C(New)消息被commit了之后，旧的leader才能正式失效，然后集群中第一个选举超时的节点将进行新的选举。

## 处理被删除的server

考虑如下的场景：一个server被从集群中删除，但是它并不知道这一情况，在选举超时过去之后，由于一直没有收到心跳消息，于是递增它的term发起了新的选举，由于这个term比当前集群的term大，于是当前集群将进行一次新的选举过程，而由于该server已经被删除，同样的在选举出新的leader之后，又会再次递增term发起新的选举....这个过程一直进行下去，会将集群经常打断进入选举状态，造成集群的不可用状态。

于是考虑增加一个新的prevote过程，一个candidate在发起选举之前，首先先发起prevote请求，咨询集群中的其他server，该candidate上的日志数据是否足够新，如果得到了超过半数节点肯定的答复，那么才正式进行选举。

然而这个过程并不能解决所有问题，如下图4.7所示：

![4.7](./pic/4.7.jpg)

S1是被删除的server，而S4是新集群的leader，S4创建了C(New)日志条目，但是还没有同步该日志给集群中的其他节点。在C(New)被提交之前，S1可能会选举超时，于是递增term发起新的选举，此时由于S1上面的日志也是最新的，所以prevote检查会通过。从这个例子中可以看出，仅依赖于日志数据的更新程度，并不足以在所有情况下阻止已经被删除的server发起选举请求干扰集群。

因此，修改RequestVote协议的处理为：如果一个server在一个选举超时内已经收到了当前leader的心跳消息，此时可以忽略RequestVote消息。



# 日志压缩

# 与客户端交互

## etcd-raft如何实现线性一致性

Linearizable Read通俗来讲，就是读请求需要读到最新的已经commit的数据，不会读到老数据。

由于所有的leader和follower都能处理客户端的读请求，所以存在可能造成返回读出的旧数据的情况：

1.  leader和follower之间存在状态差，因为follower总是由leader同步过去的，可能会返回同步之前的数据。
2.  如果发生了网络分析，某个leader实际上已经被隔离出了集群之外，但是该leader并不知道，如果还继续响应客户端的读请求，也可能会返回旧的数据。

因此，在接收到客户端的读请求时，需要保证返回的数据都是当前最新的。


### ReadOnlySafe方式
leader在接收到读请求时，需要向集群中的超半数server确认自己仍然是当前的leader，这样它返回的就是最新的数据。

在etcd-raft中，为了实现ReadOnlySafe，有如下的数据结构：

```
type ReadState struct {
    Index      uint64
    RequestCtx []byte
}
```

其中：

1.  Index：接收到该读请求时，当前节点的commit索引。
2.  RequestCtx：客户端读请求的唯一标识。

这个数据用于保存读请求到来时的节点状态。


```
type readIndexStatus struct {
   req   pb.Message
   index uint64
   acks  map[uint64]struct{}
}
```

readIndexStatus数据结构用于追踪leader向follower发送的心跳信息，其中：

1.  req：保存原始的readIndex请求。
2.  index：leader当前的commit信息。
3.  acks：存放该readIndex请求有哪些节点进行了应答。


```
type readOnly struct {
   option           ReadOnlyOption
   pendingReadIndex map[string]*readIndexStatus
   readIndexQueue   []string
}
```

readOnly用于管理全局的readIndx数据，其中：

1.  option：readOnly选项。
2.  pendingReadIndex：当前所有待处理的readIndex请求，其中key为客户端读请求的唯一标识。
3.  readIndexQueue：保存所有readIndex请求的请求唯一标识数组。

有了以上的数据结构介绍，后面是流程介绍：

1.  server收到客户端的读请求，此时会调用raft.ReadIndex函数发起一个MsgReadIndex的请求，带上的参数是客户端读请求的唯一标识。
2.  follower将向leader直接转发MsgReadIndex消息，而leader收到不论是本节点还是由其他server发来的MsgReadIndex消息，其处理都是：

    a.  首先如果该leader在成为新的leader之后没有提交过任何值，那么会直接返回不做处理。

    b.  调用r.readOnly.addRequest(r.raftLog.committed, m)保存该MsgreadIndex请求到来时的commit索引。

    c.  r.bcastHeartbeatWithCtx(m.Entries[0].Data)，向集群中所有其他节点广播一个心跳消息MsgHeartbeat，并且在其中带上该读请求的唯一标识。

    d.  follower在收到leader发送过来的MsgHeartbeat，将应答MsgHeartbeatResp消息，并且如果MsgHeartbeat消息中有ctx数据，MsgHeartbeatResp消息将原样返回这个ctx数据。

    e.  leader在接收到MsgHeartbeatResp消息后，如果其中有ctx字段，说明该MsgHeartbeatResp消息对应的MsgHeartbeat消息，是收到ReadIndex时leader消息为了确认自己还是集群leader发送的心跳消息。首先会调用r.readOnly.recvAck(m)函数，根据消息中的ctx字段，到全局的pendingReadIndex中查找是否有保存该ctx的带处理的readIndex请求，如果有就在acks map中记录下该follower已经进行了应答。

    f.  当ack数量超过了集群半数时，意味着该leader仍然还是集群的leader，此时调用r.readOnly.advance(m)函数，将该readIndex之前的所有readIndex请求都认为是已经成功进行确认的了，所有成功确认的readIndex请求，将会加入到readStates数组中，同时leader也会向follower发送MsgReadIndexResp。

    g.  follower收到MsgReadIndexResp消息时，同样也会更新自己的readStates数组信息。

    h.  readStates数组的信息，将做为ready结构体的信息更新给上层的raft协议库的使用者。


需要特别说明的是，处理读请求时，实际上leader需要确保当前自己是不是leader、该读请求对应的commit索引是否得到了半数投票，而当一个节点刚成为leader的时候，如果没有提交过任何数据，那么在它所在的这个任期（term）内的commit索引当时是并不知道的，因此在成为leader之后，需要马上提交一个no-op的空日志，这样拿到该任期的第一个commit索引。









